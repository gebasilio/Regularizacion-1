{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "%matplotlib inline\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "15000\n",
      "15000 train sequences\n",
      "10000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (15000, 400)\n",
      "x_test shape: (10000, 400)\n",
      "Build model...\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "  384/15000 [..............................] - ETA: 112406s - loss: 0.6954 - acc: 0.4844"
     ]
    }
   ],
   "source": [
    "# set parameters:\n",
    "max_features = 5000\n",
    "maxlen = 400\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "epochs = 2\n",
    "\n",
    "print('Loading data...')\n",
    "(x, y), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "# 40% del dataset para validaci√≥n\n",
    "validation = 0.40\n",
    "N_validation_split = int(x.shape[0]*(1-validation))\n",
    "print(N_validation_split)\n",
    "\n",
    "# Training Set\n",
    "x_train = x[:N_validation_split]\n",
    "y_train = y[:N_validation_split]\n",
    "\n",
    "# Cross Validation Set\n",
    "x_test = x[N_validation_split:]\n",
    "y_test = y[N_validation_split:]\n",
    "\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "model.add(Embedding(max_features,embedding_dims,input_length=maxlen))\n",
    "model.add(Dropout(0.2))\n",
    "# we add a Convolution1D, which will learn filters\n",
    "# word group filters of size filter_length:\n",
    "model.add(Conv1D(filters,kernel_size,padding='valid',activation='relu',strides=1))\n",
    "# we use max pooling:\n",
    "model.add(GlobalMaxPooling1D())\n",
    "# We add a vanilla hidden layer:\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.fit(x_train, y_train,batch_size=batch_size,epochs=epochs,\n",
    "validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"accuracy: \", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.get_weights()[0]\n",
    "biases = model.get_weights()[1]\n",
    "print(\"Pesos :\" , weights.shape)\n",
    "print(\"Biases : \" , biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from fnn_helper import PlotLosses\n",
    "checkpointer = ModelCheckpoint(filepath='model_3.imdb.hdf5', verbose=1, save_best_only=True)\n",
    "plot_losses = PlotLosses(plot_interval=1, evaluate_interval=None, x_val=x_train , y_val_categorical=y_train)\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=batch_size,epochs=epochs, validation_data=(x_test, y_test),callbacks=[plot_losses, checkpointer],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
