{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB (Internet Movie Database)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Keras Dataset](https://keras.io/datasets/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMDB Movie reviews sentiment classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Reviews de 25.000 peliculas de IMDB\n",
    "- Etiquetadas por sentimiento (Positivo/Negativo)\n",
    "- Los reviews ya estan pre-procesados\n",
    "- Cada review esta codificado como una secuencia de indices de palabras (integers)\n",
    "- Los indices estan ordenados por frecuencia. Es decir que la palabra que tiene el índice 3 es la 3er palabra mas frecuente.\n",
    "- 0 no es el indice de una palabra específica sino que se usa para codificar las palabras desconocidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import optimizers\n",
    "from fnn_helper import PlotLosses\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "15000\n",
      "15000 train sequences\n",
      "10000 val sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "Review:\n",
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
      "\n",
      "Sentimiento:\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Embedding\n",
    "max_features = 20000\n",
    "max_words = 400\n",
    "maxlen = 400\n",
    "embedding_size = 32\n",
    "\n",
    "# Convolution\n",
    "kernel_size = 3\n",
    "filters = 64\n",
    "pool_size = 4\n",
    "\n",
    "# LSTM\n",
    "lstm_output_size = 70\n",
    "\n",
    "# Training\n",
    "batch_size = 32\n",
    "epochs = 3\n",
    "\n",
    "\n",
    "#batch_size is highly sensitive.\n",
    "\n",
    "#Only 2 epochs are needed as the dataset is very small.\n",
    "\n",
    "print('Loading data...')\n",
    "\n",
    "#(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "(x, y), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "# 40% del dataset para validación\n",
    "validation = 0.40\n",
    "N_validation_split = int(x.shape[0]*(1-validation))\n",
    "print(N_validation_split)\n",
    "\n",
    "# Training Set\n",
    "x_train = x[:N_validation_split]\n",
    "y_train = y[:N_validation_split]\n",
    "\n",
    "# Cross Validation Set\n",
    "x_val = x[N_validation_split:]\n",
    "y_val = y[N_validation_split:]\n",
    "\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_val), 'val sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "print('Pad sequences (samples x time)')\n",
    "print('Review:')\n",
    "print(x_train[0])\n",
    "print()\n",
    "print('Sentimiento:')\n",
    "print(y_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "x_train shape: (15000, 400)\n",
      "x_val shape: (10000, 400)\n",
      "x_test shape: (25000, 400)\n"
     ]
    }
   ],
   "source": [
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_val = sequence.pad_sequences(x_val, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_val shape:', x_val.shape)\n",
    "print('x_test shape:', x_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 2)\n",
      "(10000, 2)\n",
      "(25000, 2)\n"
     ]
    }
   ],
   "source": [
    "num_classes = 2\n",
    "y_train_categorical = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_val_categorical = keras.utils.to_categorical(y_val, num_classes)\n",
    "y_test_categorical = keras.utils.to_categorical(y_test, num_classes)\n",
    "print(y_train_categorical.shape)\n",
    "print(y_val_categorical.shape)\n",
    "print(y_test_categorical.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model 1.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 400, 32)           640000    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 400, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 70)                28840     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 70)                0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 70)                0         \n",
      "_________________________________________________________________\n",
      "salida (Dense)               (None, 2)                 142       \n",
      "=================================================================\n",
      "Total params: 668,982\n",
      "Trainable params: 668,982\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print('Build model 1.')\n",
    "\n",
    "#Modelo 1\n",
    "model_1 = Sequential()\n",
    "model_1.add(Embedding(max_features, embedding_size, input_length=max_words))\n",
    "model_1.add(Dropout(0.2))\n",
    "model_1.add(LSTM(lstm_output_size))\n",
    "model_1.add(Dropout(0.2))\n",
    "model_1.add(Activation('relu'))\n",
    "#model.add(Dense(2))\n",
    "#model.add(Activation('sigmoid'))\n",
    "model_1.add(Dense(2, activation='softmax', kernel_initializer='normal', name='salida'))\n",
    "#model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "model_1.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model_1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint \n",
    "#Callbacks\n",
    "## Callback para graficar\n",
    "plot_losses1 = PlotLosses(plot_interval=1, evaluate_interval=20, x_val=x_val, y_val_categorical=y_val_categorical)\n",
    "## Callback para guardar pesos\n",
    "checkpointer1 = ModelCheckpoint(filepath='model_1.imdb.hdf5', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 10000 samples\n",
      "Begin training\n",
      "Epoch 1/3\n"
     ]
    }
   ],
   "source": [
    "#Entrenar el modelo \n",
    "epochs=3 #epochs=20\n",
    "model_1.fit(x_train, y_train_categorical, batch_size=batch_size, epochs=epochs, validation_data=(x_val,y_val_categorical), callbacks=[plot_losses1, checkpointer1],)\n",
    "scores_1 = model_1.evaluate(x_test, y_test_categorical, verbose=2)\n",
    "print(\"Accuracy: %.2f%%\" % (scores_1[1] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Analisis de los pesos\n",
    "weights = model_1.get_weights()[0]\n",
    "biases = model_1.get_weights()[1]\n",
    "print(weights.shape)\n",
    "print(biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cargo los valores del modelo\n",
    "model_1.load_weights('model_1.imdb.hdf5')\n",
    "score_1 = model_1.evaluate(x_test, y_test_categorical, verbose=0)\n",
    "print(\"loss: \", score_1[0])\n",
    "print(\"accuracy: \", score_1[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Build model 2.')\n",
    "#Modelo 2\n",
    "model_2 = Sequential()\n",
    "model_2.add(Embedding(max_features, embedding_size, input_length=max_words))\n",
    "model_2.add(Dropout(0.2))\n",
    "model_2.add(Activation('relu'))\n",
    "model_2.add(Dense(1, activation='sigmoid'))\n",
    "rmsprop = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=1e-3)\n",
    "model_2.add(Dense(2, activation='softmax', kernel_initializer='normal', name='salida'))\n",
    "model_2.compile(loss = 'categorical_crossentropy', optimizer=rmsprop, metrics=['accuracy'])\n",
    "print(model_2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint \n",
    "#Callbacks\n",
    "## Callback para graficar\n",
    "plot_losses2 = PlotLosses(plot_interval=1, evaluate_interval=20, x_val=x_val, y_val_categorical=y_val_categorical)\n",
    "## Callback para guardar pesos\n",
    "checkpointer2 = ModelCheckpoint(filepath='model_2.imdb.hdf5', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Entrenar el modelo \n",
    "#epochs=20\n",
    "epochs=3\n",
    "model_2.fit(x_train, y_train_categorical, batch_size=batch_size, epochs=epochs, validation_data=(x_val,y_val_categorical), callbacks=[plot_losses2, checkpointer2],)\n",
    "scores_2 = model_2.evaluate(x_test, y_test_categorical, verbose=2)\n",
    "print(\"Accuracy: %.2f%%\" % (scores_2[1] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Analisis de los pesos\n",
    "weights = model_2.get_weights()[0]\n",
    "biases = model_2.get_weights()[1]\n",
    "print(weights.shape)\n",
    "print(biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cargo los valores del modelo\n",
    "model_2.load_weights('model_2.imdb.hdf5')\n",
    "score_2 = model_2.evaluate(x_test, y_test_categorical, verbose=0)\n",
    "print(\"loss: \", score_2[0])\n",
    "print(\"accuracy: \", score_2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint \n",
    "#Callbacks\n",
    "## Callback para graficar\n",
    "plot_losses3 = PlotLosses(plot_interval=1, evaluate_interval=20, x_val=x_train, y_val_categorical=y_test)\n",
    "## Callback para guardar pesos\n",
    "checkpointer3 = ModelCheckpoint(filepath='model_3.imdb.hdf5', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model_3(input_shape, output_size, lr=0.1):\n",
    "    model = Sequential()\n",
    "    sgd = optimizers.SGD(lr=lr)\n",
    "    model.add(Dense(800, activation='sigmoid', kernel_initializer='zeros', name='middle'))\n",
    "    model.add(Dense(output_size, activation='softmax', kernel_initializer='normal', name='Salid\n",
    "    model.add(Dense(output_size, input_dim=input_shape, activation='sigmoid', kernel_initializer='normal', name='Salida' ))\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "model_3 = get_model_3(x_train.shape[1], 1)\n",
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Entrenar el modelo\n",
    "model_3.fit(x_train, y_train_categorical, epochs=50, batch_size=32, validation_data=(x_test, y_test_categorical), callbacks=[plot_losses3, checkpointer3])\n",
    "scores_3 = model_3.evaluate(x_test, y_test_categorical, verbose=2)\n",
    "print(\"Accuracy: %.2f%%\" % (scores_3[1] * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejemplo:\n",
    "<img src=\"images/training_IMDB.png\" alt=\"Drawing\" style=\"width:100%;\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
